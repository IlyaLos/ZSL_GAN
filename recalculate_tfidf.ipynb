{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for regenerating tf-idf vectors from wiki articles\n",
    "Unfortunatly, authors didn't provide tfidf vectorizer with vocab, so it's impossible to do the same transformation to input text in AttnGAN code. The best way to fix it that I can see is to recalculate tf-idf vectors. After that I'll be able to train a new ZSL_GAN encoder and use it with the same transformations in AttGan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from text_processor import TextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_descriptions = ['' for i in range(200)]\n",
    "text_processor = TextProcessor()\n",
    "\n",
    "wiki_articles_folder_path = 'data/Raw_Wiki_Articles/CUBird_Sentences'\n",
    "for class_file in os.listdir(wiki_articles_folder_path):\n",
    "    if class_file[0] not in string.digits:\n",
    "        continue\n",
    "\n",
    "    class_id = int(class_file.split('.')[0])  # 1-indexed!\n",
    "    \n",
    "    text = ''\n",
    "    for line in open(os.path.join(wiki_articles_folder_path, class_file), 'r').readlines():\n",
    "        line = line.strip()\n",
    "        start_id = 0\n",
    "        while start_id < len(line) and line[start_id] in string.digits:\n",
    "            start_id += 1\n",
    "            \n",
    "        if start_id + 1 < len(line):\n",
    "            text += line[start_id + 1:]\n",
    "    \n",
    "    final_descriptions[class_id - 1] = text_processor.preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = text_processor.get_tfidf(final_descriptions, 'out/tf_idf_vectorizer.pkl', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 7549)\n"
     ]
    }
   ],
   "source": [
    "# original article mentioned 7551 for CUB, so it's pretty close\n",
    "print(tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/CUB2011/new_tfidf.npy', tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
